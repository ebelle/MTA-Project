{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This code downloads tweets filtered by keyword. The keyword can be changed by altering q=\n",
    "in the new_tweets line below.\"\"\"\n",
    "import tweepy\n",
    "import csv\n",
    "\n",
    "consumer_key = \n",
    "consumer_secret = \n",
    "access_key = \n",
    "access_secret = \n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "\n",
    "alltweets = []\n",
    "new_tweets = tweepy.Cursor(api.search,q=\"@NYCTSubway\",count=100,lang=\"en\",since=\"2018-12-07\",tweet_mode=\"extended\").items()\n",
    "alltweets.extend(new_tweets)\n",
    "\n",
    "outtweets = [[tweet.id_str, tweet.created_at, tweet.full_text] for tweet in alltweets]\n",
    "\n",
    "with open('mta_subway_tweets.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\",\"created_at\",\"text\",\"place\",\"coordinates\"])\n",
    "    writer.writerows(outtweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function creates a list of all possible subway strings of varying lengths\"\"\"\n",
    "import itertools\n",
    "def generate_subway_strings():\n",
    "    is_subway = []\n",
    "    subway_list = [\"ace\",\"bdfm\",\"jz\",\"nrqw\",\"l\",\"s\",\"g\",\"fg\",\"em\",\"jmz\",\"ef\",\"mr\",\"bc\",\n",
    "                   \"123\",\"456\",\"7\",\"25\", \"34\"]\n",
    "    for subway in subway_list:\n",
    "        for i in range(1,5):\n",
    "            for j in itertools.combinations(subway, i):\n",
    "                subway_combination = ''.join(j)\n",
    "                if subway_combination not in is_subway:\n",
    "                    is_subway.append(subway_combination)\n",
    "    return is_subway\n",
    "\n",
    "print(generate_subway_strings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "\n",
    "class TokensAndTags:\n",
    "    \n",
    "    def __init__(self, csv_file):\n",
    "        \"\"\"Requires one arguement, a csv file\"\"\"\n",
    "        self.csv_file = csv_file\n",
    "    \n",
    "    def text_from_csv(self):\n",
    "        \"\"\"Reads the csv file and yields tweets. Assumes 5 columns. Strips apostrophes and slashes\"\"\"\n",
    "        with open (self.csv_file, \"r\") as source:\n",
    "            csv_reader = csv.reader(source)\n",
    "            for _,_,text in csv_reader:\n",
    "                text = text.replace(\"â€™\",\"\").replace(\"'\",\"\").replace(\"/\",\"\")\n",
    "                yield text\n",
    "    \n",
    "    def sentence_boundry(self):\n",
    "        \"\"\"Uses nltk sentence tokenizer to return a list of sentences\"\"\"\n",
    "        list_sentences = []\n",
    "        s = \"\".join(self.text_from_csv())\n",
    "        sentences = nltk.sent_tokenize(s)\n",
    "        for sentence in sentences:\n",
    "            if sentence in list_sentences:\n",
    "                pass\n",
    "            else:\n",
    "                list_sentences.append(sentence)\n",
    "        return list_sentences\n",
    "    \n",
    "    def make_tokens(self):\n",
    "        \"\"\"Returns a list of sentences, each of which is a list of tokens. Uses TweetTokenizer\"\"\"\n",
    "        tokens = [] \n",
    "        for sentence in self.sentence_boundry():\n",
    "            tokenized = nltk.tokenize.TweetTokenizer().tokenize(sentence)\n",
    "            for token in tokenized:\n",
    "                tokens.append(token)\n",
    "        return tokens\n",
    "    \n",
    "    def print_to_tsv(self):\n",
    "        \"\"\"Prints to tsv a list of the tokens and their tags\"\"\"\n",
    "        with open(\"tokens_and_tags.tsv\", \"w\") as sink:\n",
    "            tsv_writer = csv.writer(sink, delimiter='\\t') \n",
    "            tokens = self.make_tokens()\n",
    "            for word in tokens:\n",
    "                word = word.split()\n",
    "                for token, tag in nltk.pos_tag(word, tagset = 'universal'):\n",
    "                    tsv_writer.writerow([token,tag])\n",
    "                    \n",
    "    def parts_of_speech(self):\n",
    "        \"\"\"Returns a list of the tokens and their tags\"\"\" \n",
    "        tokens = self.make_tokens()\n",
    "        tags = []\n",
    "        for word in tokens:\n",
    "            word = word.split()\n",
    "            for token, tag in nltk.pos_tag(word, tagset = 'universal'):\n",
    "                tags.append(tag)\n",
    "        return tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_text = TokensAndTags(\"mta_subway_tweets_1_month.csv\")\n",
    "mta_text.print_to_tsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "class Find_Trains:\n",
    "    \n",
    "    def __init__(self,tsv_file):\n",
    "        \"\"\"The class takes one argument, a tsv file\"\"\"\n",
    "        self.tsv_file = tsv_file\n",
    "        \"\"\"This is a complete list of all the possible train combinations\"\"\"\n",
    "        self.is_train = self.is_train = ['a', 'c', 'e', 'ac', 'ae', 'ce', 'ace', \n",
    "                         'b', 'd', 'f', 'm', 'bd', 'bf', 'bm', 'df', 'dm', 'fm', 'bdf', 'bdm', 'bfm', 'dfm', 'bdfm', \n",
    "                         'j', 'z', 'jz', \n",
    "                         'n', 'r', 'q', 'w', 'nr', 'nq', 'nw', 'rq', 'rw', 'qw', 'nrq', 'nrw', 'nqw', 'rqw', 'nrqw', \n",
    "                         'l', 's', 'g', \n",
    "                         'fg', 'em', 'jm', 'mz', 'jmz', 'ef', 'mr', 'bc',\n",
    "                         '1', '2', '3', '12', '13', '23', '123', \n",
    "                         '4', '5', '6', '45', '46', '56', '456', \n",
    "                         '7', \n",
    "                         '25', '34']\n",
    "        \"\"\"This is only letter trains, excepting the A/a train which is dealt with below\"\"\"\n",
    "        self.is_letter_train = frozenset(['c', 'e', 'ac', 'ae', 'ce', 'ace', \n",
    "                         'b', 'd', 'f', 'm', 'bd', 'bf', 'bm', 'df', 'dm', 'fm', 'bdf', 'bdm', 'bfm', 'dfm', 'bdfm', \n",
    "                         'j', 'z', 'jz', \n",
    "                         'n', 'r', 'q', 'w', 'nr', 'nq', 'nw', 'rq', 'rw', 'qw', 'nrq', 'nrw', 'nqw', 'rqw', 'nrqw', \n",
    "                         'l', 's', 'g', \n",
    "                         'fg', 'em', 'jm', 'mz', 'jmz', 'ef', 'mr', 'bc',])\n",
    "        \"\"\"This is only number trains\"\"\"\n",
    "        self.is_number_train = frozenset(['1', '2', '3', '12', '13', '23', '123', \n",
    "                         '4', '5', '6', '45', '46', '56', '456', \n",
    "                         '7', \n",
    "                         '25', '34'])\n",
    "        \"\"\"This is only the A/a train\"\"\"\n",
    "        self.is_a_train = frozenset(['a'])\n",
    "        \"\"\"This is a list of common train nouns that are used after train characters\"\"\"\n",
    "        self.train_noun_list = [\"train\", \"trains\", \"subway\",\"line\",\"lines\", \"stop\", \"station\", \n",
    "                           \"service\", \"platform\", \"track\"]\n",
    "        \"\"\"This is a list of common train adjectives that are used before and after train characters\"\"\"\n",
    "        self.train_adj_list = frozenset([\"express\", \"exp\",\"local\",\"uptown\", \"downtown\", \"queensbound\", \n",
    "                          \"queens-bound\" \"southbound\", \"northbound\", \"southbound\", \"bound\", \n",
    "                          \"manhattan-bound\",\"brooklyn-bound\", \"bronx-bound\", \"island-bound\",\n",
    "                          \"astoria-bound\",\"sb\",\"nb\"])\n",
    "        \"\"\"This is a list of common time words to filter out number trains from times\"\"\"\n",
    "        self.times = frozenset([\"seconds\",\"minute\",\"minutes\",\"min\",\"mins\",\"hour\",\"hours\",\"hr\",\"hrs\",\n",
    "                               \"day\",\"days\",\"week\",\"weeks\",\"month\",\"months\",\"year\",\"years\",\"yrs\"])\n",
    "            \n",
    "    def make_lists(self):\n",
    "        \"\"\"This takes the tsv file and returns a list of tokens and a list of tags\"\"\"\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        with open(self.tsv_file, 'r') as source:\n",
    "            tsv_reader = csv.reader(source, delimiter='\\t')\n",
    "            for token, tag in tsv_reader:\n",
    "                tokens.append(token.casefold())\n",
    "                tags.append(tag)\n",
    "        return tokens,tags\n",
    "    \n",
    "    def find_a_trains(self):\n",
    "        \"\"\"This function finds A/a trains and excludes instances of the indefinite article 'a'\n",
    "        The following criteria are used (in order): 1) if the letter A comes after an adjective or determiner\n",
    "        except 'such' 2) if the letter A comes after another train letter, which is followed by \n",
    "        punctuation or a conjuction 3) if the token before the letter A is included in the train\n",
    "        adjective list 4) if the token after the letter A is included in the train adjective list \n",
    "        and is not followed by another train word or character\"\"\"\n",
    "        trains = []\n",
    "        tokens, tags = self.make_lists() \n",
    "        \"\"\"train_words is list which combines all the train characters and all the train nouns\"\"\"\n",
    "        train_words = self.is_train + self.train_noun_list\n",
    "        for i,token in enumerate(tokens):\n",
    "            if token in self.is_a_train: \n",
    "                if i or i<len(tokens)-1:\n",
    "                    if tags[i-1] in ['ADJ','DET'] and tokens[i-1] not in [\"such\", \"a\"]:\n",
    "                        trains.append(token)\n",
    "                    elif tags[i+1] in [\"CONJ\", \".\"] and tokens[i+2] in self.is_letter_train:\n",
    "                        trains.append(token) \n",
    "                    elif tokens[i-1] in self.train_adj_list:\n",
    "                        trains.append(token)\n",
    "                    elif tokens[i+1] in self.train_adj_list and tokens[i+2] not in train_words:\n",
    "                        trains.append(token)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "        return trains\n",
    "    \n",
    "    def find_number_trains(self):\n",
    "        \"\"\"This function finds number trains and excludes instances of integers.\n",
    "        The following criteria are used (in order): 1) if the comes before a verb 2) if the number\n",
    "        after an adjective 3) if the number comes after a train adjective 4) if the number comes\n",
    "        before a train adjective 5) if the number comes before a train noun 6) if there is another \n",
    "        train character followed by punctuation or a conjuction before the letter 7) if there is\n",
    "        punctuation or a conjuction followed by another train character after the letter\"\"\"\n",
    "        trains = []\n",
    "        tokens, tags = self.make_lists() \n",
    "        for i,token in enumerate(tokens):\n",
    "            if token in self.is_number_train:\n",
    "                if i or i<len(tokens)-1:\n",
    "                    if tags[i+1] == \"VERB\":\n",
    "                        trains.append(token)       \n",
    "                    elif tags[i-1] == \"ADJ\" and tokens[i+1] not in self.times:\n",
    "                        trains.append(token)\n",
    "                    elif tokens[i-1] in self.train_adj_list:\n",
    "                        trains.append(token)\n",
    "                    elif tokens[i+1] in self.train_adj_list:\n",
    "                        trains.append(token)\n",
    "                    elif tokens[i+1] in self.train_noun_list:\n",
    "                        trains.append(token)          \n",
    "                    elif tags[i-1] in [\"CONJ\", \".\"]:\n",
    "                        if tokens[i-2] in self.is_number_train:\n",
    "                            trains.append(token) \n",
    "                    elif tags[i+1] in [\"CONJ\", \".\"]:\n",
    "                        if tokens[i+2] in self.is_number_train:\n",
    "                            trains.append(token) \n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "        return trains\n",
    "    \n",
    "    \n",
    "    def find_letter_trains(self):\n",
    "        \"\"\"This function finds letter trains and excludes instances of other possible meanings\n",
    "        including AC = air-conditioning, BC = because, E = east, W = west or with, F = fuck, \n",
    "        and S seems to have some weird results I think due to tokenization. \"\"\"\n",
    "        trains = []\n",
    "        tokens, tags = self.make_lists() \n",
    "        for i,token in enumerate(tokens):\n",
    "            if token in self.is_letter_train:\n",
    "                if i or i<len(tokens)-1:\n",
    "                    \n",
    "                    \n",
    "                    if token == \"w\":\n",
    "                        \"\"\"disambiguates W trains from with and West street names\"\"\"\n",
    "                        if any(char.isdigit() for char in tokens[i+1]) == True:\n",
    "                            pass\n",
    "                            \"\"\"disambiguates W trains from with\"\"\"\n",
    "                        elif tags[i-1] in [\"ADJ\", \"DET\"]:\n",
    "                            trains.append(token)\n",
    "                        elif tokens[i+1] in self.train_noun_list:\n",
    "                            trains.append(token)\n",
    "                        elif tokens[i+1] in self.train_adj_list:\n",
    "                            trains.append(token)\n",
    "                        elif tokens[i-1] in self.train_adj_list:\n",
    "                            trains.append(token)\n",
    "                        else:\n",
    "                            pass    \n",
    "                    \n",
    "                    \n",
    "                    elif token == \"e\":\n",
    "                        \"\"\"disambiguates E and W trains from East and West street names\"\"\"\n",
    "                        if any(char.isdigit() for char in tokens[i+1]) == True:\n",
    "                            pass\n",
    "                        else:\n",
    "                            trains.append(token)\n",
    "                    \n",
    "                    \n",
    "                    elif token == \"ac\":\n",
    "                        \"\"\"disambiguates AC trains from air-conditioning\"\"\"\n",
    "                        if tokens[i+1] in [\"on\", \"off\", \"is\", \"blasting\"]:\n",
    "                            pass\n",
    "                        else:\n",
    "                            trains.append(token)\n",
    "                    \n",
    "                    \n",
    "                    elif token in [\"bc\", \"f\", \"s\",\"dm\"]:\n",
    "                        \"\"\"disambiguates BC trains from because, F trains from fuck and S trains \n",
    "                        from some weird results I think due to tokenization\"\"\"\n",
    "                        if tags[i-1] in [\"ADJ\", \"DET\"] and tokens[i+1] !=\"ing\" and tokens[i-2] !=\"you\":\n",
    "                            trains.append(token)\n",
    "                        elif tokens[i+1] in self.train_noun_list:\n",
    "                            trains.append(token)\n",
    "                        elif tokens[i+1] in self.train_adj_list:\n",
    "                            trains.append(token)\n",
    "                        elif tokens[i-1] in self.train_adj_list:\n",
    "                            trains.append(token)\n",
    "                        else:\n",
    "                            pass\n",
    "                    else:\n",
    "                        trains.append(token)\n",
    "        return trains\n",
    "    \n",
    "\n",
    "    \n",
    "    def compile_trains(self):\n",
    "        \"\"\"This function gathers the results of the previous 3 functions into one list\"\"\"\n",
    "        trains = []\n",
    "        for train in self.find_a_trains():\n",
    "            trains.append(train)\n",
    "        for train in self.find_letter_trains():\n",
    "            trains.append(train)\n",
    "        for train in self.find_number_trains():\n",
    "            trains.append(train)\n",
    "        return trains\n",
    "    \n",
    "    \n",
    "    def count_trains(self):\n",
    "        \"\"\"This function creates a dictionary of counts for each train.\"\"\"\n",
    "        train_counts = {}\n",
    "        for train in self.compile_trains():\n",
    "            #This splits any strings of more than one character\n",
    "            if len(train) > 1:\n",
    "                split_trains = list(train)\n",
    "                for train in split_trains:\n",
    "                    if train in train_counts:\n",
    "                        train_counts[train] += 1\n",
    "                    else:\n",
    "                        train_counts[train] = 1\n",
    "            else:\n",
    "                if train in train_counts:\n",
    "                    train_counts[train] += 1\n",
    "                else:\n",
    "                    train_counts[train] = 1\n",
    "        return train_counts\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_and_tags = Find_Trains(\"tokens_and_tags.tsv\")\n",
    "tokens_and_tags.count_trains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
